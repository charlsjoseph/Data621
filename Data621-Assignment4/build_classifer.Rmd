---
title: "Data assignment 4 : Building Classifer for Insurance data "
author: "Charls Joseph"
date: "April 25, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pROC)
library(dplyr)
library(class)  #knn
library(caret)   # confusion metric
library(knitr)
```


# Building Models


```{r}

training <- read.csv( "https://raw.githubusercontent.com/charlsjoseph/Data621/master/Data621-Assignment4/insurance_tf_train.csv")[-1]
test_set <- read.csv("https://raw.githubusercontent.com/charlsjoseph/Data621/master/Data621-Assignment4/insurance_tf_test.csv")[-1]

df_eval <- read.csv( "https://raw.githubusercontent.com/charlsjoseph/Data621/master/Data621-Assignment4/insurance_tf_eval.csv")[-1]

```

The dataset is imbalanced as we are seeing more number of negative classes than the positive classes. 

We will set 0 means "No Car crash" and 1 means "Car cash"

Considering a classifer model like this, by looking at only accuracy may not give a clear picture of model goodness. I'm assuming predicting a case whether a person whether will involve into a car crash is crucial.
As we have two types of errors( Type1(False positive) and Type2(False Negative)), we should always avoid both of these error. However in this case, we should focus more on reducing Type2(False Negative). That is predicting Negative when it is really positive so that we loss an opportunity to save a person's life and as well as claim amounts. We know sensitivity/Recall is a measure of False negative rate. So we will focus on the sensitivity than the precision or accuracy. 

Another metrics we will focuss is AUC(Area under curve measure) while comparing different classifers. 



```{r}
table(training$TARGET_FLAG)

```

We will perform 3 classifers. They are Logistic regression, KNN and Naive Bayes models. 

# Logistic Regression

```{r}

log_classifer <- glm(TARGET_FLAG ~ ., data=training[,-2], family = "binomial")
lr_pred_prob=predict(log_classifer,newdata = test_set, type ="response" )
# consider the threshold of the linear regression classifer as 0.5
lr_pred_class  <- ifelse(lr_pred_prob > 0.5, 1, 0)

roc1 <- roc(test_set$TARGET_FLAG, lr_pred_class)
roc1$auc

plot.roc(roc1,
main="Logistic Regression | ROC Curve", percent=TRUE, of="thresholds", # compute AUC (of threshold)
thresholds="best", # select the (best) threshold
print.auc = TRUE, 
print.thres="best") 

predict.lr_result <- confusionMatrix(as.factor(lr_pred_class), as.factor(test_set$TARGET_FLAG), positive = "1")
predict.lr_result
```

# KNN model

```{r}

trainData1 = training[,-c(1,2)]
testData1 = test_set[,-c(1,2)]

train_lbls <- training$TARGET_FLAG
test_lbls <- test_set$TARGET_FLAG

knn_model <- knn(train = trainData1, test = testData1, cl= train_lbls,k = 7, prob = TRUE)

roc2 <- roc(test_set$TARGET_FLAG, attributes(knn_model)$prob)
roc2$auc
plot.roc(roc2,
main="KNN Classifer | ROC Curve", percent=TRUE, of="thresholds", # compute AUC (of threshold)
thresholds="best", # select the (best) threshold
print.auc = TRUE, 
print.thres="best") 

predict.knn_result <- confusionMatrix(knn_model, as.factor(test_set$TARGET_FLAG), positive = "1")
predict.knn_result
```

# Naive Bayes

```{r}
library(e1071)

nb_model=naiveBayes(as.factor(TARGET_FLAG) ~ ., data=training[,-2])

nm_pred_prob=predict(nb_model,newdata = test_set, type =  "raw" )
nm_pred_class=predict(nb_model,newdata = test_set, type ="class")

roc3 <- roc(as.factor(test_set$TARGET_FLAG), nm_pred_prob[,2])
roc3$auc
plot.roc(roc3,
main="Naive Bayes | ROC Curve", percent=TRUE, of="thresholds", # compute AUC (of threshold)
thresholds="best", # select the (best) threshold
print.auc = TRUE, 
print.thres="best") 

predict.nb_result <- confusionMatrix(nm_pred_class, as.factor(test_set$TARGET_FLAG), positive = "1")
predict.nb_result
```

# ploting ROC of all models 

```{r}
plot.roc(roc1, print.auc = TRUE, 
                 col = "red") 
plot.roc(roc2 , add = TRUE,print.auc = TRUE, 
                 col = "green", print.auc.y = .4) 

plot.roc(roc3 , add = TRUE,print.auc = TRUE, 
                 col = "blue", print.auc.y = .6) 

legend("bottomright", legend=c("Model-1 - Logistic regression", "Model-2 - Knn", "Model-3 - Naive Bayes"),
col=c("red", "green", "blue"), lwd=2)


```


# Predicting the evaluation dataset 

As we are seeing above Naive Bayes gives a better AUC, Sensitivity and Accuracy. Hence we will use naive bayes model to predict the evaluation data set. 

```{r}

df_classifer_pred=predict(nb_model,newdata = df_eval, type ="class")
df_eval$TARGET_FLAG <- df_classifer_pred

write.csv(df_eval, "C:\\Users\\Charls\\Documents\\CunyMSDS\\Data621\\assignments\\data621\\Data621-Assignment4\\eval_results.csv")

kable(df_eval)

```
